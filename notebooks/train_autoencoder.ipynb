{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "from torch import nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "import os\n",
    "# device = torch.device(\"cuda:0\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '../data/Training Data/Normal'\n",
    "normal_subj_files = os.listdir(data_path)\n",
    "\n",
    "df_list = list()\n",
    "for idx, fname in enumerate(normal_subj_files):\n",
    "    df_temp = pd.read_csv(f'{data_path}/{normal_subj_files[0]}', index_col=0)\n",
    "    df_temp['subj'] = np.repeat(idx, len(df_temp))\n",
    "\n",
    "    df_list.append(df_temp)\n",
    "df = pd.concat(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_ann(nn.Module):\n",
    "    def __init__(self, input_size, output_size, layer_size):\n",
    "        super(model_ann, self).__init__()\n",
    "        self.input_size,  self.layer_size, self.output_size = input_size, layer_size, output_size\n",
    "\n",
    "        #List layer sizes\n",
    "        self.layer_hidden = np.concatenate([[input_size], layer_size, [output_size]])\n",
    "        \n",
    "        #Compile layers into lists\n",
    "        self.layer_list = nn.ModuleList(\n",
    "            [nn.Linear(in_features=self.layer_hidden[idx], out_features=self.layer_hidden[idx+1]) for idx in range(len(self.layer_hidden)-1)] )        \n",
    " \n",
    "    def forward(self, x):\n",
    "        #Encoding step\n",
    "        for idx in range(len(self.layer_list)):\n",
    "            x = torch.tanh(self.layer_list[idx](x))\n",
    "\n",
    "        return x\n",
    "\n",
    "class model_ann_autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, output_size, encoder_layer_size, decoder_layer_size, bottleneck = 10):\n",
    "        super(model_ann_autoencoder, self).__init__()\n",
    "        self.input_size, self.output_size = input_size, output_size\n",
    "        self.encoder_layer_size, self.decoder_layer_size = encoder_layer_size, decoder_layer_size\n",
    "\n",
    "        self.encoder = model_ann(input_size, bottleneck, layer_size=self.encoder_layer_size)\n",
    "        self.decoder = model_ann(bottleneck, input_size, layer_size=self.decoder_layer_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.encoder(x)\n",
    "        out = self.decoder(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SEE_Dataset(torch.utils.data.Dataset):\n",
    "     def __init__(self, cv_dict, fold, partition, df, device=device):\n",
    "          self.cv_dict = cv_dict\n",
    "          self.fold = fold\n",
    "          self.partition = partition\n",
    "          self.df = df\n",
    "\n",
    "          self.subj_idx = cv_dict[fold][partition]\n",
    "          self.num_subj = len(self.subj_idx) \n",
    "          self.X_tensor = self.process_dfs(self.df)\n",
    "\n",
    "     def __len__(self):\n",
    "        #'Denotes the total number of samples'\n",
    "        return self.num_subj\n",
    "\n",
    "     def process_dfs(self, df):\n",
    "          df_filtered = df[np.in1d(df['subj'], self.subj_idx)]\n",
    "          return torch.tensor(df_filtered.values).to(device)\n",
    "          \n",
    "     def __getitem__(self, slice_index):\n",
    "          return self.X_tensor[slice_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_subjects = len(normal_subj_files)\n",
    "cv_split = ShuffleSplit(n_splits=5, test_size=.25, random_state=0)\n",
    "val_split = ShuffleSplit(n_splits=1, test_size=.25, random_state=0)\n",
    "cv_dict = {}\n",
    "for fold, (train_val_idx, test_idx) in enumerate(cv_split.split(np.arange(num_subjects))):\n",
    "    for t_idx, v_idx in val_split.split(train_val_idx): #No looping, just used to split train/validation sets\n",
    "        cv_dict[fold] = {'train_idx':train_val_idx[t_idx], \n",
    "                         'test_idx':test_idx, \n",
    "                         'validation_idx':train_val_idx[v_idx]} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "num_cores = 1\n",
    "\n",
    "train_params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': num_cores, 'pin_memory':False}\n",
    "train_eval_params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': num_cores, 'pin_memory':False}\n",
    "validation_params = {'batch_size': batch_size, 'shuffle': True, 'num_workers': num_cores, 'pin_memory':False}\n",
    "test_params = {'batch_size': batch_size, 'shuffle': False, 'num_workers': num_cores, 'pin_memory':False}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/users/ntolley/.conda/envs/metanets/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/users/ntolley/.conda/envs/metanets/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/users/ntolley/.conda/envs/metanets/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_1509750/1548013941.py\", line 21, in __getitem__\n    return self.X_tensor[slice_index]\nRuntimeError: CUDA error: initialization error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/users/ntolley/grad_school/metanets/notebooks/train_autoencoder.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/grad_school/metanets/notebooks/train_autoencoder.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_x \u001b[39min\u001b[39;00m training_generator:\n\u001b[1;32m      <a href='vscode-notebook-cell://ood.ccv.brown.edu/users/ntolley/grad_school/metanets/notebooks/train_autoencoder.ipynb#X11sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/metanets/lib/python3.10/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    632\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.conda/envs/metanets/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[39mdel\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_data(data)\n",
      "File \u001b[0;32m~/.conda/envs/metanets/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     data\u001b[39m.\u001b[39;49mreraise()\n\u001b[1;32m   1373\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.conda/envs/metanets/lib/python3.10/site-packages/torch/_utils.py:722\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    718\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m    719\u001b[0m     \u001b[39m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    720\u001b[0m     \u001b[39m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    721\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 722\u001b[0m \u001b[39mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/users/ntolley/.conda/envs/metanets/lib/python3.10/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/users/ntolley/.conda/envs/metanets/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/users/ntolley/.conda/envs/metanets/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_1509750/1548013941.py\", line 21, in __getitem__\n    return self.X_tensor[slice_index]\nRuntimeError: CUDA error: initialization error\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n\n"
     ]
    }
   ],
   "source": [
    "for batch_x in training_generator:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "fold = 0\n",
    "\n",
    "# Generators\n",
    "training_set = SEE_Dataset(cv_dict, fold, 'train_idx', df)\n",
    "\n",
    "training_generator = torch.utils.data.DataLoader(training_set, **train_params)\n",
    "training_eval_generator = torch.utils.data.DataLoader(training_set, **train_eval_params)\n",
    "\n",
    "validation_set = SEE_Dataset(cv_dict, fold, 'validation_idx', df)\n",
    "validation_generator = torch.utils.data.DataLoader(validation_set, **validation_params)\n",
    "\n",
    "testing_set = SEE_Dataset(cv_dict, fold, 'test_idx', df)\n",
    "testing_generator = torch.utils.data.DataLoader(testing_set, **test_params)\n",
    "\n",
    "data_arrays = (training_set, validation_set, testing_set)\n",
    "generators = (training_generator, training_eval_generator, validation_generator, testing_generator)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper function to pytorch train networks for decoding\n",
    "def train_validate_model(model, optimizer, criterion, max_epochs, training_generator, validation_generator, device, print_freq=10, early_stop=20):\n",
    "    train_loss_array = []\n",
    "    validation_loss_array = []\n",
    "    # Loop over epochs\n",
    "    min_validation_loss, min_validation_std, min_validation_counter, min_validation_epoch = np.inf, np.inf, 0, 0\n",
    "    for epoch in range(max_epochs):\n",
    "        #___Train model___\n",
    "        model.train()\n",
    "        train_batch_loss = []\n",
    "        validation_batch_loss = []\n",
    "        for batch_x in training_generator:\n",
    "            optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "            batch_x = batch_x.float().to(device)\n",
    "\n",
    "            output = model(batch_x)\n",
    "            train_loss = criterion(output, batch_x)\n",
    "            train_loss.backward() # Does backpropagation and calculates gradients\n",
    "            optimizer.step() # Updates the weights accordingly\n",
    "\n",
    "            train_batch_loss.append(train_loss.item())\n",
    "        \n",
    "        train_loss_array.append(train_batch_loss)\n",
    "\n",
    "        #___Evaluate Model___\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            #Generate train set predictions\n",
    "            for batch_x in validation_generator:\n",
    "                batch_x = batch_x.float().to(device)\n",
    "\n",
    "                output = model(batch_x)\n",
    "                validation_loss = criterion(output, batch_x)\n",
    "\n",
    "                validation_batch_loss.append(validation_loss.item())\n",
    "\n",
    "        validation_loss_array.append(validation_batch_loss)\n",
    "\n",
    "        #Compute average loss on batch\n",
    "        train_epoch_loss = np.mean(train_batch_loss)\n",
    "        train_epoch_std = np.std(train_batch_loss)\n",
    "        validation_epoch_loss = np.mean(validation_batch_loss)\n",
    "        validation_epoch_std = np.std(validation_batch_loss)\n",
    "\n",
    "       #Check if validation loss reaches minimum \n",
    "        if validation_epoch_loss < min_validation_loss:\n",
    "            print('*',end='')\n",
    "            min_validation_loss = np.copy(validation_epoch_loss)\n",
    "            min_validation_std = np.copy(validation_epoch_std)\n",
    "            min_validation_counter = 0\n",
    "            min_validation_epoch = np.copy(epoch+1)\n",
    "\n",
    "            min_train_loss = np.copy(train_epoch_loss)\n",
    "            min_train_std = np.copy(train_epoch_std)\n",
    "            \n",
    "        else:\n",
    "            print('.',end='')\n",
    "            min_validation_counter += 1\n",
    "\n",
    "        #Print Loss Scores\n",
    "        if (epoch+1)%print_freq == 0:\n",
    "            print('')\n",
    "            print('Epoch: {}/{} ...'.format(epoch+1, max_epochs), end=' ')\n",
    "            print('Train Loss: {:.4f}  ... Validation Loss: {:.4f}'.format(train_epoch_loss,validation_epoch_loss))\n",
    "        \n",
    "        #Early stop if no validation improvement over set number of epochs\n",
    "        if min_validation_counter > early_stop:\n",
    "            print(' Early Stop; Min Epoch: {}'.format(min_validation_epoch))\n",
    "            break\n",
    "\n",
    "    loss_dict = {'min_validation_loss':min_validation_loss, 'min_validation_std':min_validation_std,'min_validation_epoch':min_validation_epoch, \n",
    "    'min_train_loss':min_train_loss, 'min_train_std':min_train_std,\n",
    "    'train_loss_array':train_loss_array, 'validation_loss_array':validation_loss_array, 'max_epochs':max_epochs}\n",
    "    return loss_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*.*******.\n",
      "Epoch: 10/1000 ... Train Loss: 279.4181  ... Validation Loss: 215.7805\n",
      "..... Early Stop; Min Epoch: 9\n"
     ]
    }
   ],
   "source": [
    "#Define hyperparameters\n",
    "lr = 1e-1\n",
    "weight_decay = 1e-4\n",
    "max_epochs = 1000\n",
    "input_size = training_set[:2].shape[1]\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "bottleneck = 10\n",
    "\n",
    "encoder_layer_size = [100, 50, 20]\n",
    "decoder_layer_size = [10, 50, 100]\n",
    "\n",
    "model = model_ann_autoencoder(input_size=input_size, output_size=input_size,\n",
    "                              encoder_layer_size=encoder_layer_size, decoder_layer_size=decoder_layer_size,\n",
    "                              bottleneck=bottleneck).to(device)\n",
    "\n",
    "# Define Loss, Optimizerints h\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "#Train model\n",
    "loss_dict = train_validate_model(model, optimizer, criterion, max_epochs, training_generator, validation_generator, device, 10, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "metanets",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
